================================================================================
PREDICTIVE AWS COST OPTIMIZATION: A FINOPS APPROACH USING PROPHET FORECASTING
================================================================================

MBA PROJECT REPORT (CONDENSED ~4,000 WORDS)

Submitted by: Mohammed Abdul Basith | SSODL (Symbiosis School of Distance Learning)
Date: October 23, 2025

================================================================================
ABSTRACT (≤250 words)
================================================================================

Purpose: This study tests whether predictive analytics can enable proactive AWS cost control. With ~35% of cloud spend typically wasted, we integrate Prophet forecasting with FinOps practices to cut waste, plan capacity seasonally, and guide governance.

Methodology: We analyzed 218 AWS billing snapshots over 1,057 days for EC2, RDS, S3, Lambda, and ECS across three regions using Prophet. Model validation used an 80–20 temporal split and 10 accuracy metrics (MAPE, sMAPE, WAPE, MAE, RMSE, MdAPE, R², Bias, PICP). We paired this with 9 FinOps KPIs (Waste Rate, Unit Cost, Budget Variance, RI/SP Coverage & Utilization, Tag Compliance, CAGR, Volatility, Seasonality Strength). We also implemented anomaly detection and rightsizing opportunity estimation.

Findings: Forecasting achieved MAPE 6.50%, R² 0.919, Bias −0.32% (excellent/planning-grade). Waste totaled ₹290,558 of ₹1,270,055 (22.88%). Seasonality is clear: Q4 spikes, Q2 dips. Tag compliance reached 83% (→90% target), RI/SP coverage 22.48% (utilization 90.79%), monthly rightsizing savings ₹19,819, and 21 anomalies (₹146,332) were detected. Annual savings potential exceeds ₹3.5–5.0 lakh.

Limitations: Simulated yet benchmark-calibrated data; AWS-only; confidence intervals under-cover (PICP 77.27% vs. 93–97% ideal); unpredictable external events not modeled.

Practical Implications: Replace static budgets with forecasts; align capacity to seasons; raise RI/SP coverage; enforce tagging; act on rightsizing; monitor anomalies. 

Originality/Value: A practitioner-ready, validated Predictive FinOps framework—accurate, accessible, and measurable—bridging academic rigor and operational value.

Keywords: FinOps, AWS Cost, Prophet, Forecasting, Waste, Rightsizing, RI/SP, Tagging, MAPE, Budget Variance, Seasonality, Anomaly Detection

================================================================================
1. INTRODUCTION
================================================================================

The public cloud promises elasticity, speed, and a shift from capital expenditure (CapEx) to operational expenditure (OpEx). In practice, that flexibility also introduces financial volatility. Pricing differs by service, region, and purchase option (on‑demand, reserved instances, savings plans, spot). Data transfer and storage policies add further complexity. As environments scale, many organizations see “cost creep,” where spend rises faster than usage or revenue, eroding unit economics and obscuring return on cloud investments.

Industry benchmarks consistently estimate that roughly one‑third of cloud spend delivers little or no value. Common drivers include idle or oversized compute, unattached storage volumes, underutilized databases, and non‑production environments left running 24×7. The issue is not only technical; it is organizational. Finance, engineering, and operations often act in silos, with monthly cost surprises, weak ownership (missing or inconsistent tags), and limited use of commitment discounts that require forecasting confidence.

Financial Operations for cloud (FinOps) emerged as a cultural and procedural answer. The FinOps Foundation frames a maturity journey—Inform (visibility), Optimize (efficiency), and Operate (continuous governance). Many enterprises achieve visibility but stall before automation and predictive planning. A durable solution must be both technically sound and socially adoptable: easy to understand, measurable, and directly linked to actions that teams can own.

This project proposes a Predictive FinOps framework that combines time‑series forecasting with governance and optimization levers. We apply Prophet to daily AWS costs to quantify trend and seasonality, produce forward‑looking budgets with uncertainty, and validate accuracy using a comprehensive metric suite. We then integrate FinOps KPIs—waste rate, unit cost, commitment coverage and utilization, tag compliance, budget variance, and anomaly frequency—so forecasts convert into operational decisions such as rightsizing, commitment planning, and seasonal scaling.

The guiding questions are pragmatic: Can we forecast with planning‑grade accuracy? How much waste can be prevented with rightsizing and tagging? What commitment coverage balances savings and risk? Do seasonal spikes justify autoscaling and pre‑positioned capacity? And can alerts and cadenced reviews keep savings from eroding over time?

For a mid‑sized spend (₹1.27 crore over the study window), our results indicate material impact. Forecast accuracy reaches MAPE 6.50% with R² 0.919 and minimal bias (−0.32%), providing confidence to replace static budgets with rolling forecasts. Quantified waste is 22.88% (₹290,558), tag compliance is 83% (on a path to ≥90%), and RI/SP coverage is 22.48% with high utilization (90.79%), suggesting safe expansion. Combined levers—rightsizing, commitment discounts, seasonal scaling, and anomaly prevention—indicate annual savings potential of ₹3.5–5.0 lakh while increasing cost predictability and accountability.

In short, the contribution is a validated, practitioner‑ready, and repeatable approach that moves cloud finance from reactive reporting to proactive planning—without requiring deep data‑science specialization.

================================================================================
2. LITERATURE REVIEW
================================================================================

Cloud Cost Management and Governance: Early cloud economics literature underscored elasticity and pay‑per‑use benefits while warning of cost unpredictability as adoption scales. Traditional pre‑migration calculators assisted with TCO estimates but did not solve ongoing optimization. More recent surveys converge on four pillars for mature cost practice: visibility (allocation and showback/chargeback), efficiency (rightsizing, scheduling, storage hygiene), financial instruments (reserved instances and savings plans), and governance (tagging, policies, and processes). Despite abundant tools, most organizations remain reactive due to fragmented ownership and weak predictive capabilities.

FinOps Practice and Maturity: The FinOps Foundation’s framework (Inform → Optimize → Operate) has diffused widely across enterprise cloud programs. Studies and industry reports show common plateaus at the Inform stage—where teams create dashboards yet struggle to institutionalize optimization and continuous guardrails. Barriers include skills gaps, tool complexity, and incentives that do not reward cost‑aware design. Empirical evidence suggests that teams succeed when they adopt lightweight rituals (monthly reviews, ownership by tag, automated alerts) and when KPIs are both business‑relevant and technically actionable.

Time‑Series Forecasting for Cloud Costs: Classical models such as ARIMA and exponential smoothing can perform well on stationary series but require careful diagnostics, transformations, and expert tuning. Machine learning approaches (gradient boosting, random forests) and deep learning (LSTM, TCN) may fit complex patterns but demand substantial data volume and risk overfitting, especially with fewer than 1,000 observations. Prophet, introduced by Facebook/Meta, targets business time series with piecewise linear or logistic trends and multiple seasonalities, providing interpretable components, robust defaults, and tolerance to missing data and outliers—traits well aligned with daily cloud cost signals that exhibit yearly seasonality and trend changes due to optimization programs.

Optimization Instruments and Economic Levers: Research and practitioner guides identify idle/oversized resources and storage misuse as dominant waste drivers. Rightsizing—selecting smaller instance types or storage classes—consistently ranks among the most impactful, low‑risk levers when supported by utilization telemetry. Commitment discounts (RIs and Savings Plans) offer 30–70% reductions but require accurate demand forecasts and utilization discipline; over‑commitment leads to sunk costs, while under‑commitment leaves savings on the table. Seasonal planning remains under‑leveraged despite observable patterns tied to business cycles and releases.

Anomaly Detection and Guardrails: Unplanned cost spikes often arise from misconfigurations, failed jobs, or traffic surges. Statistical and ML‑based anomaly detection enhances responsiveness, yet false positives and lack of process integration can cause alert fatigue. Successful implementations pair calibrated thresholds with ownership routing (by tag) and review cadences so anomalies are triaged quickly and learning is captured.

Metrics, Uncertainty, and Decision‑Readiness: Forecast assessments commonly report MAPE, RMSE, and MAE; fewer report bias, which matters for commitments, or interval coverage (PICP), which matters for budgeting risk. Decision‑readiness improves when uncertainty is explicit and calibrated; otherwise, teams either overtrust point forecasts or ignore them. In parallel, the literature on financial management emphasizes unit economics (cost per request/transaction) to connect spend with value, yet this is under‑represented in academic cloud cost work relative to the prevalence of such metrics in practice.

Synthesis and Gap: The literature offers many point solutions—forecasting, anomaly detection, rightsizing, governance—but fewer integrated approaches that connect accurate, interpretable prediction to FinOps KPIs and day‑to‑day operating rhythms. This project addresses that integration gap and emphasizes accessibility: a setup that a business analyst can run, verify, and explain, increasing the odds of sustained adoption.

================================================================================
3. RESEARCH GAPS & OBJECTIVES
================================================================================

Identified Gaps:
- Integration Gap: Forecasting, optimization tactics, and governance are often treated as separate projects, reducing impact and accountability.
- Accessibility Gap: Many published methods assume specialist skills, complex feature engineering, or deep learning infrastructure that typical FinOps teams lack.
- Validation Gap: Studies frequently report MAPE alone, omitting bias and interval coverage (PICP), and sometimes use random splits that leak temporal information.
- KPI Gap: Academic work under‑emphasizes FinOps KPIs such as waste, unit cost, tag compliance, and commitment coverage/utilization that drive real decisions.
- Seasonality Gap: The magnitude of seasonal variance and its operational implications (budgeting, autoscaling, commitments) are rarely quantified together.
- Operationalization Gap: Few papers describe lightweight processes—alerts, ownership by tag, monthly rituals—that convert insights into sustained outcomes.

Objectives:
- O1 (Accuracy and Calibration): Build a forecasting pipeline using Prophet that achieves MAPE ≤15% (target ≤10%), R² ≥0.85, bias within ±5%, and transparent interval coverage (PICP) suitable for budgeting risk decisions.
- O2 (Integrated KPIs): Pair forecasts with a compact FinOps KPI set—waste rate, unit cost, tag compliance, RI/SP coverage and utilization, budget variance, rightsizing savings, and anomaly frequency—to bridge data science and financial governance.
- O3 (Operational Playbook): Deliver an adoption roadmap (roles, reviews, alert thresholds, and metrics) that a business analyst can own with minimal engineering friction.
- O4 (Value Quantification): Estimate savings from rightsizing, seasonal scaling, commitments, and anomaly prevention; target ≥₹3,00,000 annualized at study scale.

Research Questions:
1) What level of forecast accuracy and calibration (MAPE, R², Bias, PICP) is achievable for daily AWS costs with limited history (~1,000 days)?
2) How strong are seasonal effects and how should they influence budgets, autoscaling, and commitments across Q2 troughs and Q4 peaks?
3) How much waste exists by service/region, and what is the safe rightsizing and commitment expansion path given utilization?
4) Which governance gaps (tags, ownership) most constrain optimization and how quickly can they be improved?
5) What quantified savings and variance reductions result from implementing the proposed playbook over 3–6 months?

Hypotheses:
- H1: Prophet, with minimal tuning, delivers planning‑grade forecasts (MAPE ≤10%, R² ≥0.90) on this class of cost data.
- H2: Seasonal differences exceed 20% between Q4 and Q2, warranting proactive scaling.
- H3: An integrated program (rightsizing + commitments + seasonality + anomaly control) yields ≥₹3,00,000 annual savings, with additional benefits in predictability and accountability.

================================================================================
4. SCOPE & LIMITATIONS
================================================================================

Scope:
- Time Horizon: 1,057 historical days with a 365‑day forecast horizon to cover annual business cycles.
- Services and Regions: EC2, RDS, S3, Lambda, and ECS across us‑east‑1, us‑west‑2, and ap‑south‑1—representing the majority of compute and storage spend.
- Organizational Profile: Mid‑sized program with annualized spend in the ₹1–2 crore range; results intended to generalize to similar scales.
- Methods and Metrics: Prophet forecasting with interpretable components; 10 accuracy metrics (MAPE, sMAPE, WAPE, MAE, RMSE, MdAPE, R², Bias/Bias%, PICP); FinOps KPIs (waste, unit cost, tag compliance, RI/SP coverage/utilization, budget variance, rightsizing, anomalies).
- Deliverables: Reproducible code, dashboards for day‑to‑day management, a metrics pack for governance, and a 3–6 month adoption playbook.

Limitations:
- Data Realism: The dataset is simulated but calibrated against industry benchmarks and realistic usage patterns; real‑world idiosyncrasies may differ.
- Cloud Coverage: AWS‑only; extension to multi‑cloud is feasible but requires service and pricing normalization.
- Cost Dimensions: Some cost drivers (network egress, marketplace, support) are not modeled in detail and should be included for full production roll‑out.
- Forecast Horizon: Accuracy degrades beyond 12 months; quarterly retraining is recommended.
- Exogenous Shocks: Campaigns, incidents, or policy changes can dominate short‑term patterns and are not predictable from historical cost alone.
- Interval Calibration: Observed PICP is 77.27% versus an ideal 93–97% for 95% intervals; production use should include interval widening or quantile calibration.

================================================================================
5. RESEARCH METHODOLOGY & DATA ANALYSIS
================================================================================

Overall Design: We adopt a quantitative, time‑series approach centered on Prophet for forecasting and a compact KPI suite for operationalization. The workflow comprises six phases: data preparation, exploratory analysis, model building, validation and calibration, KPI computation, and visualization/operationalization.

Data and Feature Construction: The dataset contains 218 aggregated daily snapshots spanning 1,057 days. Each snapshot consolidates service‑ and region‑level costs into a total daily cost signal. To reflect realistic behavior, the synthetic generator imposes: (a) a long‑run trend decrease (~40% over the period) capturing cumulative optimization; (b) yearly seasonality (±30%) reflecting business cycles; and (c) stochastic noise (σ≈8%) to emulate operational variability. Auxiliary fields support FinOps analysis: cpu_utilization, active vs. idle cost, environment (prod/non‑prod), tag completeness, RI/SP coverage and utilization, rightsizing opportunities and savings, budgets and variances, anomaly markers, request counts, and derived unit cost (₹ per 1K requests).

Tools and Environment: Python 3.13; pandas and numpy for data manipulation; Prophet for forecasting; matplotlib and seaborn for visualizations. The environment and dependencies are minimal by design to ease adoption by analytics teams.

Forecasting Model: Prophet is configured with yearly seasonality and a piecewise linear trend. The model outputs point forecasts (yhat) and uncertainty intervals (yhat_lower, yhat_upper). We split the series temporally (80% train, 20% test) to mimic realistic forecasting and avoid leakage. The trained model produces a 365‑day forward forecast to cover a full annual cycle.

Accuracy and Calibration Metrics: We compute 10 metrics to triangulate forecast quality: MAPE, sMAPE, WAPE, MAE, RMSE, MdAPE, R², Bias/Bias%, and PICP (prediction interval coverage probability). Business thresholds are set as follows—MAPE ≤15% (≤10% excellent), R² ≥0.85, bias within ±5%, and PICP near 95% for 95% intervals. These thresholds translate analytic performance into decision readiness (e.g., is it safe to use forecasts for commitments and budgets?).

Computation of FinOps KPIs: In parallel, we calculate governance and optimization KPIs: waste rate (idle + overprovisioned costs / total), tag compliance (% of costs with owner/environment tags), commitment coverage (share under RI/SP) and utilization (share of committed usage actually consumed), unit cost (₹ per 1K requests), budget variance (actual vs. budget), rightsizing savings (sum of actionable recommendations), and anomaly frequency/impact (count and total value of spikes beyond calibrated thresholds).

Anomaly Detection: We flag anomalies when actual cost breaches the upper forecast bound (yhat_upper) by a configured margin. Each event is logged with date, magnitude, and responsible team via tags to support triage and post‑mortems. In the study window we detect 21 anomalies totaling ₹146,332.

Visualization and Decision Aids: We produce component plots (trend and seasonality), a forecast vs. actual chart with intervals, and an 11‑panel FinOps dashboard including cost trends, distribution by service/region, CPU utilization heatmaps, tagging status, commitment coverage/utilization, rightsizing pipeline and realized savings, budget variance, unit cost trends, and a KPI summary panel. These views underpin monthly reviews and make ownership explicit.

Ethics and Reproducibility: The study uses no real client data. All assumptions, parameters, and code are documented for replication. The approach is intentionally lightweight so that finance or operations analysts can run it without specialist infrastructure.

Model Configuration Details: Prophet is initialized with yearly seasonality enabled and default changepoint detection to capture slope shifts as optimization programs take effect. We use additive seasonality (appropriate for cost levels in INR), allow automatic changepoint placement (with a modest changepoint prior to avoid overfitting), and retain the default holiday settings (none) since explicit business events are not encoded. Hyperparameters are chosen to favor interpretability and stability over marginal gains in fit.

Evaluation Formulas (informal): We define MAPE as the average absolute percentage error between actuals and forecasts; sMAPE symmetrically scales errors to reduce sensitivity near zero; WAPE weights errors by total demand; MAE and RMSE quantify absolute and squared deviations; MdAPE is the median percentage error, robust to outliers; R² assesses explained variance; Bias/Bias% capture systematic over‑ or under‑prediction; and PICP measures the fraction of actuals falling inside the stated prediction interval—a key indicator of interval calibration. Together, these metrics balance accuracy, robustness, and risk awareness.

Data Quality and Preprocessing: We validate time monotonicity, fill small gaps with linear interpolation, and remove obvious outliers only when corroborated by tags or incident notes. We normalize tags (owner, environment) to canonical values to ensure consistent ownership routing. Rightsizing candidates are filtered with guardrails (e.g., sustained low utilization and minimal burst risk) to avoid performance regressions. All transformations are logged to support auditability and reproducibility.

Threats to Validity: The use of simulated yet benchmark‑calibrated data may under‑represent rare edge cases (e.g., sudden architectural shifts). Results reflect the chosen service mix and may vary with data‑heavy or network‑heavy workloads. Finally, our interval under‑coverage (PICP 77.27%) means naive use of 95% bands could underestimate risk; interval widening or quantile calibration is recommended before using intervals for budget commitments.

================================================================================
6. FINDINGS AND DISCUSSION
================================================================================

1) Forecast Accuracy is Planning‑Grade: The model achieves MAPE 6.50% and R² 0.919 with a small negative bias (−0.32%). This accuracy is sufficient for rolling budgets, commitment planning, and early‑warning systems. The out‑of‑sample results suggest stability across different spending regimes (peak vs. trough months).

2) Trend Decline Reflects Ongoing Optimization: A net ~40% downward trend across the study period indicates that prior optimization initiatives—rightsizing, storage hygiene, and demand shaping—have compounded. This highlights a positive feedback loop: better visibility and commitments enable confident scaling and further optimization.

3) Seasonality is Pronounced and Actionable: Q4 months show consistent spend spikes relative to Q2 troughs, exceeding the 20% threshold in most years. Translating this into action, teams should scale capacity (and budgets) ahead of Q4 and proactively dial down non‑production and flexible workloads entering Q2. Autoscaling policies and scheduled turn‑downs can capture these gains with low risk.

4) Waste Represents a Material Savings Pool: Estimated waste is 22.88% (₹290,558 of ₹1,270,055). RDS contributes a disproportionate share due to overprovisioned instances and idle read replicas. A focused rightsizing campaign and storage class review can quickly reduce waste below the 15% target.

5) Utilization Levels are Healthy: Average CPU utilization is 78.5%, with zero instances below 40%. This suggests an optimization culture that values efficiency and reduces the risk of performance regressions from aggressive rightsizing. It also supports a safe increase in commitment coverage.

6) Governance is Improving but Not Yet Mature: Tag compliance is 83% against a target of ≥90%. Missing owner or environment tags impede accountability and slow remediation for anomalies. A lightweight tagging policy with auto‑remediation (e.g., quarantine or notifications) can close this gap within one quarter.

7) Commitment Strategy Should Prioritize Coverage Growth: RI/SP coverage sits at 22.48% but utilization of existing commitments is strong at 90.79%. This profile argues for expanding coverage to 60–80% over the next two quarters, focusing on stable, production workloads with predictable baselines.

8) Anomaly Detection Adds a Safety Net: We identify 21 anomalies totaling ₹146,332. Most are single‑day spikes associated with batch jobs or misconfigurations. Routing alerts to owners by tag and reviewing weekly reduces mean time to detection and prevents budget erosion.

9) Unit Economics Enable Value Conversations: A derived unit cost of ₹57.73 per 1K requests bridges engineering and finance discussions, enabling product teams to evaluate pricing, promotions, and performance trade‑offs with a consistent economic yardstick.

10) Intervals Under‑Cover and Should be Calibrated: PICP is 77.27% under a nominal 95% interval, indicating intervals are too narrow for risk management uses. In production, widen intervals (e.g., use 97.5% quantiles) or apply post‑hoc calibration to align coverage with policy.

================================================================================
7. RECOMMENDATIONS
================================================================================

Immediate (0–30 days):
- Seasonal Scaling: Pre‑position capacity ahead of Q4 and schedule turn‑downs entering Q2; enable autoscaling for elastic workloads and enforce start/stop schedules for non‑production.
- Rightsizing Sprint: Execute the top 20 rightsizing actions (prioritize RDS and EC2) to capture ₹19,819/month; validate performance with canary rollouts and rollback criteria.
- Guardrails and Reviews: Deploy anomaly alerts for breaches of yhat_upper with routing by owner tag; begin weekly “forecast vs. actual” checks and a simple action log to track outcomes.

Short‑Term (1–3 months):
- Commitment Expansion: Increase RI/SP coverage from 22% to 60–80%, targeting stable, 24×7 production baselines; ensure utilization remains ≥90% via periodic coverage reviews.
- Tagging and Ownership: Enforce policy that all production spend must include owner and environment tags; remediate untagged resources through automation and monthly scorecards.
- Rolling Budgets: Replace static monthly budgets with rolling 90‑day forecasts and tolerance bands based on calibrated intervals; escalate when breaches exceed set thresholds.

Medium to Long Term (3–6 months):
- Institutionalize FinOps: Establish monthly cost reviews with service owners, publish KPI scorecards, and adopt cost‑aware design patterns in architecture reviews.
- Model Enhancements: Add business regressors (launch schedules, campaigns), retrain quarterly, and calibrate intervals to improve PICP for budgeting.
- Scope Expansion: Incorporate networking and data transfer, additional services with rising spend, and sustainability KPIs (e.g., estimated carbon per request) to align with ESG goals.

Expected Impact: Combined actions are projected to save ₹3.5–5.0 lakh annually at current scale, reduce budget variance by 15–20%, and embed a culture of proactive cost stewardship.

================================================================================
8. CONCLUSION
================================================================================

We develop and validate a Predictive FinOps framework that is both analytically rigorous and operationally simple. Using Prophet with minimal tuning, we achieve planning‑grade accuracy (MAPE 6.50%, R² 0.919, Bias −0.32%). We pair forecasts with a focused set of FinOps KPIs—waste, unit cost, tagging, commitments, variance, rightsizing, anomalies—to ensure that predictions convert into accountable actions.

The framework moves organizations from reactive cost reporting to proactive planning. It turns seasonality into a capacity plan, waste into a rightsizing backlog, forecasts into commitment coverage decisions, and anomalies into routable incidents. Importantly, it is designed for adoption by business analysts and service owners without requiring deep data‑science expertise.

While the study uses simulated but benchmark‑calibrated data and focuses on AWS, the approach generalizes: add business regressors, calibrate intervals for budgeting risk, and extend to multi‑cloud via service mapping. Deployed as a monthly operating rhythm with automated alerts, the framework can deliver sustained savings on the order of ₹3.5–5.0 lakh annually at current scale and greater absolute value as spend grows.

================================================================================
9. KEY METRICS SNAPSHOT
================================================================================

Forecast accuracy: MAPE 6.50% | R² 0.919 | Bias −0.32% | PICP 77.27%
Waste: 22.88% (₹290,558 of ₹1,270,055) | Rightsizing: ₹19,819/month
Governance: Tag compliance 83% (target ≥90%)
Commitments: RI/SP coverage 22.48% (target 60–80%); utilization 90.79%
Operations: Avg CPU 78.5% | 0 instances <40% | 97 instances >70%
Anomalies: 21 events totaling ₹146,332
Unit economics: ₹57.73 per 1K requests
Estimated annual savings: ₹3.5–5.0 lakh

================================================================================
END OF CONDENSED REPORT (~4,000 words)
================================================================================
